{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKMeoHjWN-3k"
   },
   "source": [
    "# Assignment 8: DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACUkHex3N-3m"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_eassay (TFIDF)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_eassay (TFIDF W2V)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (best `depth` in range [1, 5, 10, 50], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>n_estimators</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n",
    "        <ul>\n",
    "            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n",
    "            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n",
    "            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n",
    "        </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "   <li><b>Task 2: </b>For this task consider set-1 features. Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n",
    "   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM), you need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n",
    "  Note: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n",
    "  </li>\n",
    "    <br>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6FUMj5TN-3y"
   },
   "source": [
    "<h1>1. Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQmid3VAN-31"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqY4ES_3N-33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109248, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "\n",
       "    clean_categories                 clean_subcategories  \\\n",
       "0       math_science  appliedsciences health_lifescience   \n",
       "1       specialneeds                        specialneeds   \n",
       "2  literacy_language                            literacy   \n",
       "3    appliedlearning                    earlydevelopment   \n",
       "4  literacy_language                            literacy   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  \n",
       "2  having class 24 students comes diverse learner...  329.00  \n",
       "3  i recently read article giving students choice...  481.04  \n",
       "4  my students crave challenge eat obstacles brea...   17.74  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv('preprocessed_data.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lXoccSzN-37"
   },
   "source": [
    "<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSw1_vFcN-38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73196, 8) (73196,)\n",
      "(36052, 8) (36052,)\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "## Import required library \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## create dependent and independent varaible\n",
    "y_df = data['project_is_approved'].values\n",
    "X_df = data.drop('project_is_approved',axis=1)\n",
    "\n",
    "## create train,test split \n",
    "X_tr,X_test,y_tr,y_test = train_test_split(X_df, y_df, test_size=0.33, random_state=42,stratify=y_df)\n",
    "print(X_tr.shape, y_tr.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNXtHNGVN-4B"
   },
   "source": [
    "<h2>1.3 Make Data Model Ready: encoding eassay, and project_title</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEevUQOcN-4C"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF_essays_sub(X_train, y_train, X_test, y_test, col_name = 'essay'):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4),max_features=500)\n",
    "    vectorizer.fit(X_train[col_name].values) # fit has to happen only on train data\n",
    "\n",
    "    # we use the fitted CountVectorizer to convert the text to vector\n",
    "    X_train_ohe =   vectorizer.transform(X_train[col_name].values)\n",
    "    X_test_ohe = vectorizer.transform(X_test[col_name].values)\n",
    "\n",
    "    print(\"After vectorizations {}\".format(col_name))\n",
    "    print(X_train_ohe.shape, y_train.shape)\n",
    "    print(X_test_ohe.shape, y_test.shape)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    return X_train_ohe, X_test_ohe, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations essay\n",
      "(73196, 500) (73196,)\n",
      "(36052, 500) (36052,)\n",
      "====================================================================================================\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_tr_essay_tfidf, X_test_essay_tfidf,TFIDF_feature_name = TFIDF_essays_sub(X_tr, y_tr, X_test, y_test, col_name = 'essay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using Pretrained Model - TFIDF weighted W2V\n",
    "# Train Essays\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit(X_tr[\"essay\"])\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_vec.get_feature_names(), list(tfidf_vec.idf_)))\n",
    "tfidf_words = set(tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73196/73196 [07:53<00:00, 154.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73196\n",
      "Wall time: 7min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_tr_essay_wghtd_w2vec = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_tr[\"essay\"]): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_tr_essay_wghtd_w2vec.append(vector)\n",
    "\n",
    "print(len(X_tr_essay_wghtd_w2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36052/36052 [02:21<00:00, 254.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36052\n",
      "Wall time: 2min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_essay_wghtd_w2vec = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test[\"essay\"]): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_test_essay_wghtd_w2vec.append(vector)\n",
    "\n",
    "print(len(X_test_essay_wghtd_w2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9B9fYus1N-4I"
   },
   "source": [
    "<h2>1.4 Make Data Model Ready: encoding numerical, categorical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-yI_TVhN-4J"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding \n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(X_train, y_train, X_test, y_test,col_name):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(X_train[col_name].values) # fit has to happen only on train data\n",
    "\n",
    "    # we use the fitted CountVectorizer to convert the text to vector\n",
    "    X_tr_ohe = vectorizer.transform(X_train[col_name].values)\n",
    "    X_test_ohe = vectorizer.transform(X_test[col_name].values)\n",
    "\n",
    "    print(\"After vectorizations {}\".format(col_name))\n",
    "    print(X_tr_ohe.shape, y_train.shape)\n",
    "    print(X_test_ohe.shape, y_test.shape)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    return X_tr_ohe, X_test_ohe, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations school_state\n",
      "(73196, 51) (73196,)\n",
      "(36052, 51) (36052,)\n",
      "['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn', 'mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj', 'nm', 'nv', 'ny', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_state_ohe, X_test_state_ohe, state_feature_name = one_hot_encoding(X_tr, y_tr, X_test, y_test,'school_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations project_grade_category\n",
      "(73196, 4) (73196,)\n",
      "(36052, 4) (36052,)\n",
      "['grades_3_5', 'grades_6_8', 'grades_9_12', 'grades_prek_2']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_pgc_ohe, X_test_pgc_ohe, pgc_feature_name = one_hot_encoding(X_tr, y_tr, X_test, y_test,'project_grade_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations teacher_prefix\n",
      "(73196, 5) (73196,)\n",
      "(36052, 5) (36052,)\n",
      "['dr', 'mr', 'mrs', 'ms', 'teacher']\n",
      "====================================================================================================\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "X_tr_tpr_ohe, X_test_tpr_ohe, tp_feature_name = one_hot_encoding(X_tr, y_tr, X_test, y_test,'teacher_prefix')\n",
    "print(len(tp_feature_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations clean_categories\n",
      "(73196, 9) (73196,)\n",
      "(36052, 9) (36052,)\n",
      "['appliedlearning', 'care_hunger', 'health_sports', 'history_civics', 'literacy_language', 'math_science', 'music_arts', 'specialneeds', 'warmth']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_cln_catg_ohe, X_test_cln_catg_ohe, cln_cat_feature_name = one_hot_encoding(X_tr, y_tr, X_test, y_test,'clean_categories') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations clean_subcategories\n",
      "(73196, 30) (73196,)\n",
      "(36052, 30) (36052,)\n",
      "['appliedsciences', 'care_hunger', 'charactereducation', 'civics_government', 'college_careerprep', 'communityservice', 'earlydevelopment', 'economics', 'environmentalscience', 'esl', 'extracurricular', 'financialliteracy', 'foreignlanguages', 'gym_fitness', 'health_lifescience', 'health_wellness', 'history_geography', 'literacy', 'literature_writing', 'mathematics', 'music', 'nutritioneducation', 'other', 'parentinvolvement', 'performingarts', 'socialsciences', 'specialneeds', 'teamsports', 'visualarts', 'warmth']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_cln_sub_catg_ohe, X_test_cln_sub_catg_ohe, cln_sub_catg_feature_name = one_hot_encoding(X_tr, y_tr, X_test, y_test,'clean_subcategories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize your data \n",
    "def norm_data(X_tr,y_tr, X_test, y_test, col_name = 'price'):\n",
    "    \n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    normalizer.fit(X_tr['price'].values.reshape(1,-1))\n",
    "\n",
    "    X_tr_norm = normalizer.transform(X_tr['price'].values.reshape(-1,1))\n",
    "    X_test_norm = normalizer.transform(X_test['price'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "    print(\"After vectorizations\")\n",
    "    print(X_tr_norm.shape, y_tr.shape)\n",
    "    print(X_test_norm.shape, y_test.shape)\n",
    "    print(\"=\"*100)\n",
    "    return X_tr_norm,X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_price_norm, X_test_price_norm = norm_data(X_tr, y_tr, X_test, y_test,'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_tr_nopp_norm, X_test_nopp_norm = norm_data(X_tr, y_tr, X_test, y_test,'teacher_number_of_previously_posted_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73196, 500)\n",
      "(73196, 51)\n",
      "(73196, 4)\n",
      "(73196, 5)\n",
      "(73196, 9)\n",
      "(73196, 30)\n",
      "(73196, 1)\n",
      "(73196, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr_essay_tfidf.shape)\n",
    "print(X_tr_state_ohe.shape)\n",
    "print(X_tr_pgc_ohe.shape)\n",
    "print(X_tr_tpr_ohe.shape)\n",
    "print(X_tr_cln_catg_ohe.shape)\n",
    "print(X_tr_cln_sub_catg_ohe.shape)\n",
    "print(X_tr_price_norm.shape)\n",
    "print(X_tr_nopp_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix\n",
      "(73196, 601) (73196,)\n",
      "(36052, 601) (36052,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "## prepare your data set to train your model\n",
    "from scipy.sparse import hstack\n",
    "X_tr_tfidf_csr = hstack((X_tr_essay_tfidf,X_tr_state_ohe, X_tr_pgc_ohe, X_tr_tpr_ohe, X_tr_cln_catg_ohe,X_tr_cln_sub_catg_ohe,X_tr_nopp_norm,X_tr_price_norm)).tocsr()\n",
    "X_test_tfidf_csr = hstack((X_test_essay_tfidf,X_test_state_ohe, X_test_pgc_ohe, X_test_tpr_ohe, X_test_cln_catg_ohe,X_test_cln_sub_catg_ohe,X_test_nopp_norm,X_test_price_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr_tfidf_csr.shape, y_tr.shape)\n",
    "print(X_test_tfidf_csr.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix\n",
      "(73196, 401) (73196,)\n",
      "(36052, 401) (36052,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "## prepare your data set to train your model\n",
    "from scipy.sparse import hstack\n",
    "X_tr_wghtd_w2v_csr = hstack((X_tr_essay_wghtd_w2vec,X_tr_state_ohe, X_tr_pgc_ohe, X_tr_tpr_ohe, X_tr_cln_catg_ohe,X_tr_cln_sub_catg_ohe,X_tr_nopp_norm,X_tr_price_norm)).tocsr()\n",
    "X_test_wghtd_w2v_csr = hstack((X_test_essay_wghtd_w2vec,X_test_state_ohe, X_test_pgc_ohe, X_test_tpr_ohe, X_test_cln_catg_ohe,X_test_cln_sub_catg_ohe,X_test_nopp_norm,X_test_price_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr_wghtd_w2v_csr.shape, y_tr.shape)\n",
    "print(X_test_wghtd_w2v_csr.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFZ2BOVKN-4N"
   },
   "source": [
    "<h2>1.5 Appling  Decision Tree on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply  Decision Tree on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag95KYbvN-4O"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Perform grid search cv to tune parameter for max_dept and min_sample_split\n",
    "\n",
    "hyper_parameter = {'max_depth': [1, 5, 10, 50, 100, 500, 100], \\\n",
    "                  'min_samples_split': [5, 10, 100, 500]}\n",
    "\n",
    "dtclf = DecisionTreeClassifier(class_weight='balanced') \n",
    "clf = GridSearchCV(dtclf, hyper_parameter, cv=10, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "clf.fit(X_tr_tfidf_csr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc= clf.cv_results_['mean_train_score']\n",
    "train_auc_std = clf.cv_results_['std_train_score']\n",
    "test_auc = clf.cv_results_['mean_test_score'] \n",
    "test_auc_std = clf.cv_results_['std_test_score']\n",
    "\n",
    "#Output of GridSearchCV\n",
    "print('Best score: ',clf.best_score_)\n",
    "print('Best Hyper parameters: ',clf.best_params_)\n",
    "print('='*75)\n",
    "print('Train AUC scores')\n",
    "print(clf.cv_results_['mean_train_score'])\n",
    "print('CV AUC scores')\n",
    "print(clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zqgP8b0N-4U"
   },
   "source": [
    "<h2>1.6 Getting top features using `feature_importances_`</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHvBlI_9N-4X"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfyyOdUaN-4e"
   },
   "source": [
    "<h1>2. Summary</h1>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "9_Assignment_DT_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
